# Data sets and models {#dataSetsIntro}

We illustrate the methods presented in this book by using two datasets: 
   
* Predicting odds of survival out of *Sinking of the RMS Titanic* 
* Predicting prices for *Apartments in Warsaw* 

The first dataset will be used to illustrate the application of the techniques in the case of a predictive model for a binary dependent variable. The second one will provide an example for models for a continuous variable.

In this chapter, we provide a short description of each of the datasets, together with results of exploratory analyses. We also introduce models that will be used for illustration purposes in subsequent chapters. 

## Sinking of the RMS Titanic {#TitanicDataset}

![Titanic sinking by Willy St√∂wer](figure/Titanic.jpg)

Sinking of the RMS Titanic is one of the deadliest maritime disasters in history (during peacetime). Over 1500 people died as a consequence of collision with an iceberg. Projects like *Encyclopedia titanica* `https://www.encyclopedia-titanica.org/` are a source of rich and precise data about Titanic's passengers. 
The `stablelearner` package includes a data frame with some passenger characteristics. 
The dataset, after some data cleaning and variable transformations, is also available in the `DALEX` package. In particular, the `titanic` data frame contains 2207 observations (for 1317 passengers and 890 crew members) and nine variables:

* *gender*, person's (passenger's or crew member's) gender, a factor (categorical variable) with two levels (categories) `male` (78%) and `female` (22%);
* *age*, person's age in years, a numerical variable; the age is given in (integer) years, range 0 -- 74 years; 
* *class*, the class in which the passenger travelled, or the duty class of a crew member; a factor with seven levels: `1st` (14.7%), `2nd` (12.9%), `3rd` (32.1%), `deck crew` (3%), `engineering crew` (14.7%), `restaurant staff` (3.1%), `victualling crew` (19.5%);
* *embarked*, the harbor in which the person embarked on the ship, a factor with four levels, `Belfast` (8.9%), `Cherbourg` (12.3%), `Queenstown` (5.6%), `Southampton` (73.2%);
* *country*, person's home country, a factor with 48 levels, the most common are `England` (51%), `United States` (12%), `Ireland` (6.2%) and `Sweden` (4.8%);
* *fare*, the price of the ticket (only available for passengers; 0 for crew members), a numerical variable range 0 -- 512;
* *sibsp*, the number of siblings/spouses aboard the ship, a numerical variable range 0 -- 8;
* *parch*, the number of parents/children aboard the ship, a numerical variable range 0 -- 9;
* *survived*, a factor with two levels `yes` (67.8%), `no` (32.2%), indicating whether the person survived or not.

The R code below provides more info about the contents of the dataset, values of the variables, etc.

```{r, warning=FALSE, message=FALSE}
library("DALEX")
head(titanic, 2)
```

Models considered for this dataset will use *survived* as the (binary) dependent variable. 

### Data exploration {#exploration-titanic}

It is always advisable to explore data before modelling. However, as this book is focused on model exploration, we will limit the data exploration part.

Before exploring the data, we first do some pre-processing. In particular, the value of variables *age*, *country*, *sibsp*, *parch*, and *fare* is missing for a limited number of observations (2, 81, 10, 10, and 26, respectively). Analyzing data with missing values is a topic on its own  (Little and Rubin 1987; Schafer 1997; Molenberghs and Kenward 2007). An often-used approach is to impute the missing values. Toward this end, multiple imputation should be considered (Schafer 1997; Molenberghs and Kenward 2007; van Buuren 2012). However, given the limited number of missing values and the intended illustrative use of the dataset, we will limit ourselves to, admittedly inferior, single imputation. In particular, we replace the missing *age* values by the mean of the observed ones, i.e., 30. Missing *country* will be coded by "X". For *sibsp* and *parch*, we replace the missing values by the most frequently observed value, i.e., 0. Finally, for *fare*, we use the mean fare for a given *class*, i.e., 0 pounds for crew, 89 pounds for the 1st, 22 pounds for the 2nd, and 13 pounds for the 3rd class. The R code presented below implements the imputation steps.

* missing `age` is replaced by its average, that is 30

```{r, warning=FALSE, message=FALSE}
titanic$age[is.na(titanic$age)] = 30
```
* missing `country` is replaced by `"X"`

```{r, warning=FALSE, message=FALSE}
titanic$country <- as.character(titanic$country)
titanic$country[is.na(titanic$country)] = "X"
titanic$country <- factor(titanic$country)
```

* missing `fare` is replaced by within `class` average, that is 89, 22 and 13 correspondingly

```{r, warning=FALSE, message=FALSE}
titanic$fare[is.na(titanic$fare) & titanic$class == "1st"] = 89
titanic$fare[is.na(titanic$fare) & titanic$class == "2nd"] = 22
titanic$fare[is.na(titanic$fare) & titanic$class == "3rd"] = 13
```

* missing `sibsp` and `parch` are replaced by 0

```{r, warning=FALSE, message=FALSE}
titanic$sibsp[is.na(titanic$sibsp)] = 0
titanic$parch[is.na(titanic$parch)] = 0
```

After imputing the missing values, we investigate the association between survival status and other variables. Most variables in the Titanic dataset are categorical, except Age and Fare. In order to keep the exploration uniform we first transformed them into categorical variables. Figure \@ref(fig:titanicExplorationHistograms) shows histograms for both variables. Age is discretized into 5 categories with cutoffs 5, 10, 20 and 30 while Fare is discretized with cutoffs 1, 10, 25, and 50.

Figures \@ref(fig:titanicExplorationGenderAge)-\@ref(fig:titanicExplorationCountry) present graphically the proportion non- and survivors for different levels of the other variables with the use of mosaic plots. The height of the bars (on the y-axis) reflects the marginal distribution (proportions) of the observed levels of the variable. On the other hand, the width of the bars (on the x-axis) provides the information about the proportion of non- and survivors. Note that, to construct the graphs for *age* and *fare*, we categorized the range of the observed values.

Figure \@ref(fig:titanicExplorationGenderAge) indicates that the proportion of survivors was larger for females and children below 5 years of age. This is most likely the result of the "women and children first" principle that is often evoked in situations that require evacuation of persons whose life is in danger. The principle can, perhaps, partially explain the trend seen in Figure \@ref(fig:titanicExplorationParch), i.e., a higher proportion of survivors among those with 1-3 parents/children and 1-2 siblings/spouses aboard. Figure \@ref(fig:titanicExplorationClass) indicates that passengers travelling in the first and second class had a higher chance of survival, perhaps due to the proximity of the location of their cabins to the deck. Interestingly, the proportion of survivors among crew deck was similar to the proportion of the first-class passengers. It also shows that the proportion of survivors increased with the fare, which is consistent with the fact that the proportion was higher for passengers travelling in the first and second class. Finally, Figure \@ref(fig:titanicExplorationCountry) does not suggest any noteworthy trends.        

```{r titanicExplorationHistograms, warning=FALSE, message=FALSE, echo=FALSE, fig.width=11, fig.height=6, fig.cap="Histogram of Age and Fare for the Titanic data.", out.width = '100%', fig.align='center'}
library("ggplot2")
library("ggmosaic")
library("patchwork")
library("forcats")
titanic$age_cat   <- cut(titanic$age, c(0,5,10,20,30,100))
titanic$parch_cat <- cut(titanic$parch, c(-1, 0,1,2, 100), labels = c("0", "1", "2", ">3"))
titanic$sibsp_cat <- cut(titanic$sibsp, c(-1, 0,1,2, 100), labels = c("0", "1", "2", ">3"))
titanic$country_cat <- fct_lump(titanic$country, 8)
titanic$fare_cat  <- cut(titanic$fare, c(-1,0,10,25,50,520), c("0","1-10","10-24","25-50",">50"), include.lowest = TRUE)

pl01 <- ggplot(titanic, aes(age)) +
  geom_histogram(binwidth = 5, color = "white") + 
  theme_drwhy() + ggtitle("Histograms for Age")

pl02 <- ggplot(titanic, aes(fare)) +
  geom_histogram(binwidth = 10, color = "white") + 
  theme_drwhy() + ggtitle("Histograms for Fare")

pl01 + pl02
```


```{r titanicExplorationGenderAge, warning=FALSE, message=FALSE, echo=FALSE, fig.width=11, fig.height=6, fig.cap="Survival status in groups defined be Gender and Age for the Titanic data.", out.width = '100%', fig.align='center'}
pl1 <- ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, gender), fill=survived)) +
   labs(x="Gender", y="Survived?", title='Survival per Gender') + theme_drwhy() + theme(legend.position = "none", panel.grid = element_blank()) +  scale_fill_manual(values = colors_discrete_drwhy(2))

pl2 <- ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, age_cat), fill=survived)) +
   labs(x="Age", y="Survived?", title='Survival per Age') + theme_drwhy() + theme(legend.position = "none", panel.grid = element_blank()) + scale_fill_manual(values = colors_discrete_drwhy(2))

pl1 + pl2
```

```{r titanicExplorationParch, warning=FALSE, message=FALSE, echo=FALSE, fig.width=11, fig.height=6, fig.cap="Survival according to the number of parents/children and siblings/spouses in the Titanic data.", out.width = '100%', fig.align='center'}
pl3 <- ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, parch_cat), fill=survived)) +
   labs(x="Number of Parents/Children Aboard", y="Survived?", title='Survival per n.o. Parents/Children') + theme_drwhy() + theme(legend.position = "none", panel.grid = element_blank()) + scale_fill_manual(values = colors_discrete_drwhy(2))

pl4 <- ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, sibsp_cat), fill=survived)) +
   labs(x="Number of Siblings/Spouses Aboard", y="Survived?", title='Survival per n.o. Siblings/Spouses') + theme_drwhy() + theme(legend.position = "none", panel.grid = element_blank()) +  scale_fill_manual(values = colors_discrete_drwhy(2))

pl3 + pl4
```

```{r titanicExplorationClass, warning=FALSE, message=FALSE, echo=FALSE, fig.width=11, fig.height=6, fig.cap="Survival according to the class and port of embarking in the Titanic data.", out.width = '100%', fig.align='center'}
pl5 <- ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, class), fill=survived)) +
   labs(x="Passenger class", y="Survived?", title='Survival  per Class') + theme_drwhy() + theme(legend.position = "none", axis.text.x = element_text(angle = 90), panel.grid = element_blank()) + scale_fill_manual(values = colors_discrete_drwhy(2))

pl6 <- ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, embarked), fill=survived)) +
   labs(x="Embarked", y="Survived?", title='Survival per Embarking Harbor') + theme_drwhy() + theme(legend.position = "none", axis.text.x = element_text(angle = 90), panel.grid = element_blank()) + scale_fill_manual(values = colors_discrete_drwhy(2))

pl5 + pl6
```

```{r titanicExplorationCountry, warning=FALSE, message=FALSE, echo=FALSE, fig.width=11, fig.height=6, fig.cap="Survival according to fare and country in the Titanic data.", out.width = '100%', fig.align='center'}
pl7 <- ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, fare_cat), fill=survived)) +
   labs(x="Fare", y="Survived?", title='Survival per Fare group') + theme_drwhy() + theme(legend.position = "none", panel.grid = element_blank()) + scale_fill_manual(values = colors_discrete_drwhy(2))

pl8 <- ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, country_cat), fill=survived)) +
   labs(x="Country", y="Survived?", title='Survival per Country') + theme_drwhy() + theme(legend.position = "none", axis.text.x = element_text(angle = 90), panel.grid = element_blank()) + scale_fill_manual(values = colors_discrete_drwhy(2))

pl7 + pl8
```


### Logistic regression model {#model-titanic-lmr}

The dependent variable of interest, *survival*, is binary. Thus, a natural choice is to start the predictive modelling with logistic regression model. As there is no reason to expect a linear relationship between age and odds of survival, we use linear tail-restricted cubic splines, available in the `rcs()` function of the `rms` package [@rms], to model the effect of age. We also do not expect linear relation for the `fare` variable, but because of it's skewness, we do not use splines for this variable. The results of the model are stored in model-object `titanic_lmr_v6`, which will be used in subsequent chapters. 
 
```{r, warning=FALSE, message=FALSE}
library("rms")
set.seed(1313)
titanic_lmr_v6 <- lrm(survived == "yes" ~ gender + rcs(age) + class +
         sibsp + parch + fare + embarked, titanic)
titanic_lmr_v6
```

Note that our prime interest is not in the assessment of model performance, but rather in the understanding of model behavior. This is why we do not split the data into train/test subsets. The model is trained and will be explained on the whole dataset.


### Random forest model {#model-titanic-rf}

As a challenger to the logistic regression model, we consider a random forest model. Random forest is known for good predictive performance, is able to grasp low-order variable interactions, and is quite stable [@randomForestBreiman]. To fit the model, we apply the `randomForest()` function, with default settings, from the package with the same name [@randomForest].  

In the first instance, we fit a model with the same set of explanatory variables as the logistic regression model. The results of the model are stored in model-object `titanic_rf_v6`.

```{r titanicRandomForest01, warning=FALSE, message=FALSE}
library("randomForest")
set.seed(1313)
titanic_rf_v6 <- randomForest(survived ~ class + gender + age + sibsp + 
         parch + fare + embarked, data = titanic)
titanic_rf_v6
```

For comparison purposes, we also consider a model with only three explanatory variables: *class*, *gender*, and *age*. The results of the model are stored in model-object `titanic_rf_v3`.

```{r titanicRandomForest02, warning=FALSE, message=FALSE}
titanic_rf_v3 <- randomForest(survived ~ class + gender + age, 
         data = titanic)
titanic_rf_v3
```
   
### Gradient boosting model {#model-titanic-gbm}

Let's consider an another challenger -- the gradient-boosting model [@Friedman00greedyfunction]. The tree based boosting models are known for being able to accommodate higher-order interactions between variables. We use the same set of six explanatory variables as for the logistic regression model. To fit the gradient-boosting model, we use function `gbm()` from the `gbm` package [@gbm]. The results of the model are stored in model-object `titanic_gbm_v6`.

```{r titanicGBM01, warning=FALSE, message=FALSE}
library("gbm")
set.seed(1313)
titanic_gbm_v6 <- gbm(survived == "yes" ~ class + gender + age + sibsp + 
         parch + fare + embarked, data = titanic, n.trees = 15000, 
         distribution = "bernoulli")
titanic_gbm_v6
```

   
### Support Vector Machine model {#model-titanic-svm}

Finally, we consider also Support Vector Machine model [@svm95vapnik]. We use the C-classification mode. To fit the Support Vector Machine model, we use function `svm()` from the `e1071` package [@e1071]. The results of the model are stored in model-object `titanic_svm_v6`.

```{r titanicSVM01, warning=FALSE, message=FALSE}
library("e1071")
set.seed(1313)
titanic_svm_v6 <- svm(survived == "yes" ~ class + gender + age + sibsp +
            parch + fare + embarked, data = titanic, 
            type = "C-classification", probability = TRUE)
titanic_svm_v6
```

### Model predictions {#predictions-titanic}

Let us now compare predictions that are obtained from the three different models. In particular, we will compute the predicted probability of survival for an 8-year-old boy who embarked in Belfast and travelled in the 1-st class with no parents nor siblings and with a ticket costing 72 pounds. 

First, we create a dataframe `johny_d` that contains the data describing the passenger.

```{r titanicPred01, warning=FALSE, message=FALSE}
johny_d <- data.frame(
            class = factor("1st", levels = c("1st", "2nd", "3rd", "deck crew",
                        "engineering crew", "restaurant staff", "victualling crew")),
            gender = factor("male", levels = c("female", "male")),
            age = 8,
            sibsp = 0,
            parch = 0,
            fare = 72,
            embarked = factor("Southampton", levels = c("Belfast",
                        "Cherbourg","Queenstown","Southampton"))
)
```

Subsequently, we use the generic function `predict()` to get the predicted probability of survival for the logistic regression model. 

```{r, warning=FALSE, message=FALSE}
(pred_lmr <- predict(titanic_lmr_v6, johny_d, type = "fitted"))
```
The predicted probability is equal to `r round(pred_lmr, 2)`.

We do the same for the random forest and gradient boosting models. 

```{r, warning=FALSE, message=FALSE}
(pred_rf <- predict(titanic_rf_v6, johny_d, type = "prob"))
(pred_gbm <- predict(titanic_gbm_v6, johny_d, type = "response", n.trees = 15000))
```

As a result, we obtain the predicted probabilities of `r round(pred_rf[1,2], 2)` and `r round(pred_gbm, 2)`, respectively.

The models lead to different probabilities. Thus, it might be of interest to understand the reason for the differences, as it could help us to decide which of the predictions we might want to trust. 

Note that for some examples we will use another observation (instance) with lower chances of survival. Let's call this passenger Henry.

```{r, warning=FALSE, message=FALSE}
henry <- data.frame(
         class = factor("1st", levels = c("1st", "2nd", "3rd", "deck crew", 
                     "engineering crew", "restaurant staff", "victualling crew")),
         gender = factor("male", levels = c("female", "male")),
         age = 47,
         sibsp = 0,
         parch = 0,
         fare = 25,
         embarked = factor("Cherbourg", levels = c("Belfast",
                           "Cherbourg","Queenstown","Southampton"))
)
predict(titanic_lmr_v6, henry, type = "fitted")
predict(titanic_rf_v6, henry, type = "prob")
predict(titanic_gbm_v6, henry, type = "response", n.trees = 15000)
```

### Model adapters {#ExplainersTitanicRCode}

Model-objects created with different machine learning libraries may have different internal structures. Thus, first, we have got to create an adapter for the model that provides an uniform interface. Toward this end, we use the `explain()` function from the `DALEX` package [@DALEX]. The function requires five arguments: 

* `model`, a model-object;
* `data`, a validation data frame; 
* `y`, observed values of the dependent variable for the validation data; 
* `predict_function`, a function that returns prediction scores; if not specified, then a default `predict()` function is used;
* `label`, an unique name of the model; if not specified, then it is extracted from the `class(model)`. 

Each adapter contains all elements needed to create a model explanation, i.e., a suitable `predict()` function, validation data set, and the model object. Thus, in subsequent chapters we will use the explainers instead of the model objects to keep code snippets more concise. 

```{r, warning=FALSE, message=FALSE, eval=FALSE}
explain_titanic_lmr_v6 <- explain(model = titanic_lmr_v6, 
                                 data = titanic[, -9],
                                 y = titanic$survived == "yes", 
                                 label = "Logistic Regression")
explain_titanic_lmr_v6$model_info$type = "classification"
explain_titanic_rf_v6 <- explain(model = titanic_rf_v6, 
                                 data = titanic[, -9],
                                 y = titanic$survived == "yes", 
                                 label = "Random Forest")
explain_titanic_rf_v3 <- explain(model = titanic_rf_v3, 
                                 data = titanic[, -9],
                                 y = titanic$survived == "yes", 
                                 label = "Random Forest small")
explain_titanic_gbm_v6 <- explain(model = titanic_gbm_v6, 
                                 data = titanic[, -9],
                                 y = titanic$survived == "yes", 
                                 label = "Generalized Boosted Regression")
explain_titanic_svm_v6 <- explain(model = titanic_svm_v6, 
                                 data = titanic[, -9],
                                 y = titanic$survived == "yes", 
                                 label = "Support Vector Machine")
```

```{r eval=FALSE, echo=FALSE}
# saveToLocalRepo(model_titanic_lmr, repoDir = "models")
# "56d8a46955e91f0472243e1af8021b96"
# saveToLocalRepo(explain_titanic_lmr_v6, repoDir = "models")
# "ff1cd6221c34ea70a9e033b5725c9585"

# saveToLocalRepo(titanic_rf_v6, repoDir = "models")
# "31570ec57a3b72d3ec83a5f9b22cbaaa"
# saveToLocalRepo(explain_titanic_rf_v6, repoDir = "models")
# "6ed54968790fbbe291acceb7dd6bc2ad"

# saveToLocalRepo(titanic_rf_v3, repoDir = "models")
# "855c117e1d08e793b820da14ccfdc7a5"
# saveToLocalRepo(explain_titanic_rf_v3, repoDir = "models")
# "5b32a9ed8ce5a1d63833706dbe38e221"

# saveToLocalRepo(titanic_gbm_v6, repoDir = "models")
# "0854469e60467cb25c3b7c48da5fd3dd"
# saveToLocalRepo(explain_titanic_gbm_v6, repoDir = "models")
# "87271254388a263c90ef3fa3a7a806ee"

# saveToLocalRepo(titanic_svm_v6, repoDir = "models")
# "be26e200fd6453088f5db791ac07471c"
# saveToLocalRepo(explain_titanic_svm_v6, repoDir = "models")
# "21966045bff86bba565814e8f1a18384"

```

### List of objects for the `titanic` example  {#ListOfModelsTitanic}

In the previous sections we have built four predictive models for the `titanic` data set. The models will be used in the rest of the book to illustrate the model explanation methods and tools. 

For the ease of reference, we summarize the models in Table \@ref(tab:archivistHooksOfModelsTitanic). The binary model-objects can be downloaded by using the indicated `archivist` hooks [@archivist]. By calling a function specified in the last column of the table, one can restore a selected model in its local R environment.

Table: (\#tab:archivistHooksOfModelsTitanic) Predictive models created for the `titanic` dataset. 

| Model name   | Model generator | Variables  | Archivist hooks |
|--------------|-----------------|------------|-----------------|
| `titanic_lmr_v6`  | `rms:: lmr` v.5.1.3  | gender, age, class, sibsp, parch, fare, embarked |  Get the model: `archivist:: aread("pbiecek/models/56d8a")`. Get the explainer: `archivist:: aread("pbiecek/models/ff1cd")` |
| `titanic_rf_v6`  | `randomForest:: randomForest`  v.4.6.14 | gender, age, class, sibsp, parch, fare, embarked | Get the model:  `archivist:: aread("pbiecek/models/31570")`. Get the explainer: `archivist:: aread("pbiecek/models/6ed54")` |
| `titanic_rf_v3`  | `randomForest:: randomForest`  v.4.6.14 | gender, age, class  | Get the model:  `archivist:: aread("pbiecek/models/855c1")`. Get the explainer: `archivist:: aread("pbiecek/models/5b32a")` |
| `titanic_gbm_v6`  | `gbm:: gbm`  v.2.1.5 | gender, age, class, sibsp, parch, fare, embarked | Get the model:  `archivist:: aread("pbiecek/models/08544")`. Get the explainer: `archivist:: aread("pbiecek/models/87271")` |
| `titanic_svm_v6`  | `e1071:: svm`  1.7-2 | gender, age, class, sibsp, parch, fare, embarked | Get the model:  `archivist:: aread("pbiecek/models/be26e")`. Get the explainer: `archivist:: aread("pbiecek/models/21966")` |

Table \@ref(tab:archivistHooksOfDataFramesTitanic) summarizes the data frames that will be used in examples in the subsequent chapters.

Table: (\#tab:archivistHooksOfDataFramesTitanic) Data frames created for the `titanic` example. 

| Description  | No. rows | Variables  | Link to this object |
|--------------|----------|------------|---------------------|
| `titanic` dataset with imputed missing values  | 2207  | gender, age, class, embarked, country, fare, sibsp, parch, survived |  `archivist:: aread("pbiecek/models/27e5c")` |
| `johny_d` 8-year-old boy that travelled in the 1st class without parents  | 1 | class, gender, age, sibsp, parch, fare, embarked  |  `archivist:: aread("pbiecek/models/e3596")` |
| `henry` 47-year-old male passenger from the 1st class, paid 25 pounds and embarked at Cherbourg  | 1 | class, gender, age, sibsp, parch, fare, embarked |  `archivist:: aread("pbiecek/models/a6538")`  |


## Apartment prices {#ApartmentDataset}

![Warsaw skyscrapers by Artur Malinowski Flicker](figure/am1974_flicker.jpg)

Predicting house prices is a common exercise used in machine-learning courses. Various datasets for house prices are available at websites like Kaggle (https://www.kaggle.com) or UCI Machine Learning Repository (https://archive.ics.uci.edu). 

In this book, we will work with an interesting variant of this problem. The `apartments` dataset is an artificial dataset created to match key characteristics of real apartments in Warsaw, the capital of Poland. However, the dataset is created in a way that two very different models, namely linear regression and random forest, have almost exactly the same accuracy. The natural question is then: which model should we choose? We will show that the model-explanation tools provide important insight into the key model characteristics and are helpful in model selection.

The dataset is available in the `DALEX` package [@DALEX]. It contains 1000 observations (apartments) and six variables:

* *m2.price*, apartments price per meter-squared (in EUR), a numerical variable range 1607 -- 6595;
* *construction.year*, the year of construction of the block of flats in which the apartment is located, a numerical variable range 1920 -- 2010;
* *surface*, apartment's total surface in square meters, a numerical variable range 20 -- 150;
* *floor*, the floor at which the apartment is located (ground floor taken to be the first floor), a numerical integer variable with values from 1 to 10;
* *no.rooms*, the total number of rooms, a numerical  variable with values from 1 to 6;
* *district*, a factor with 10 levels indicating the district of Warsaw where the apartment is located.

The R code below provides more info about the contents of the dataset, values of the variables, etc.

```{r, warning=FALSE, message=FALSE}
library("DALEX")
head(apartments, 2)
```

Models considered for this dataset will use *m2.price* as the (continuous) dependent variable.

Model predictions will be obtained for a set of six apartments included in data frame `apartments_test`.

```{r, warning=FALSE, message=FALSE}
head(apartments_test)
```

### Data exploration {#exploration-apartments}

Note that `apartments` is an artificial dataset created to illustrate and explain differences between random forest and linear regression. Hence, the structure of the data, the form and strength of association between variables, plausibility of distributional assumptions, etc., is better than in a real-life dataset. In fact, all these characteristics of the data are known. Nevertheless, we conduct some data exploration to illustrate the important aspects of the data.

The variable of interest is *m2.price*, the price per meter-squared. The histogram presented in Figure  \@ref(fig:appartmentsExplorationMi2) indicates that the distribution of the variable is slightly skewed to the right. 
```{r appartmentsExplorationMi2, warning=FALSE, message=FALSE, echo=FALSE, fig.width=6, fig.height=4, fig.cap="Distribution of the price per meter-squared in the apartments data."}
ggplot(data = apartments) +
   geom_histogram(aes(m2.price), binwidth = 100, color = "white") +
   labs(x="Price per meter-squared", title='Histogram for apartments prices') + 
   theme_drwhy() + theme(legend.position = "none") 
```

Figure  \@ref(fig:appartmentsMi2Construction) suggests (possibly) a nonlinear relation between *construction.year* and *m2.price* and a linear relation between *surface* and *m2.price*.


```{r appartmentsMi2Construction, warning=FALSE, message=FALSE, echo=FALSE, fig.width=11, fig.height=6, fig.cap="Left panel shows apartment price per m2 vs. year of construction, right panel shows price  vs. square footage", out.width = '100%', fig.align='center'}
pa1 <- ggplot(data = apartments, aes(construction.year, m2.price)) +
   geom_point(size = 0.3) +
  geom_smooth(se = FALSE, size=1, color = "#371ea3") +
   labs(y="Price per meter-squared", x = "Construction year", 
   title='Apartment price per m2 vs. construction year') + theme_drwhy() +
   theme(legend.position = "none") 

pa2 <- ggplot(data = apartments, aes(surface, m2.price)) +
   geom_point(size = 0.3) +
  geom_smooth(se = FALSE, size=1, color = "#371ea3") +
   labs(y="Price per meter-squared", x = "Surface (meter-squared)", 
        title='Apartment price per m2 vs. surface') + theme_drwhy() + 
   theme(legend.position = "none") 

pa1 + pa2
```

Relation between *floor* and *m2.price* is also close to linear, as well as relation between *no.rooms* and *m2.price* as seen in Figure \@ref(fig:appartmentsMi2Floor).


```{r appartmentsMi2Floor, warning=FALSE, message=FALSE, echo=FALSE, fig.width=11, fig.height=6, fig.cap="Price per meter-squared vs. floor and vs. number of rooms.", out.width = '100%', fig.align='center'}
pa3 <- ggplot(data = apartments, aes(floor, m2.price)) +
    geom_boxplot(aes(group = floor), se = FALSE, size=0.5, fill = "#371ea3", color = "white", alpha=0.3) +
    geom_jitter(size = 0.3, width = 0.15, height = 0) +
#    geom_smooth(se = FALSE, size=1, color = "#371ea3") +
    labs(y="Price per meter-squared", x = "Floor", title='Apartment price per m2 vs. floor') + theme_drwhy() + theme(legend.position = "none")  + scale_x_continuous(breaks = 1:10)

pa4 <- ggplot(data = apartments, aes(no.rooms, m2.price, group = no.rooms)) +
    geom_boxplot(aes(group = no.rooms), se = FALSE, size=0.5, fill = "#371ea3", color = "white", alpha=0.3) +
   geom_jitter(size = 0.3, width = 0.15, height = 0) +
   labs(y="Price per meter-squared", x = "Number of rooms", title='Apartment price per m2 vs. number of rooms') + theme_drwhy() + theme(legend.position = "none") + scale_x_continuous(breaks = 1:6)

pa3 + pa4
```

Surface and number of rooms are correlated and prices depend on district. Boxplots plots in Figure \@ref(fig:appartmentsSurfaceNorooms) indicate that the highest prices per meter-squared are observed in Srodmiescie (Downtown).

```{r appartmentsSurfaceNorooms, warning=FALSE, message=FALSE, echo=FALSE, fig.width=11, fig.height=6, fig.cap="Left panel: surface vs. number of rooms. Right panel: price per meter-squared for different districts", out.width = '100%', fig.align='center'}
pa5 <- ggplot(data = apartments, aes(no.rooms, surface, group = no.rooms)) +
    geom_boxplot(aes(group = no.rooms), se = FALSE, size=0.5, fill = "#371ea3", color = "white", alpha=0.3) +
   geom_jitter(size = 0.3, width = 0.15, height = 0) +
   labs(y="Surface (meter-squared)", x = "Number of rooms", title='Relation between no rooms and price per square meter') + theme_drwhy() + theme(legend.position = "none")  + scale_x_continuous(breaks = 1:6)

apartments$district <- reorder(apartments$district, apartments$m2.price, mean)
pa6 <- ggplot(data = apartments, aes(district, m2.price)) +
    geom_boxplot(aes(group = district), se = FALSE, size=0.5, fill = "#371ea3", color = "white", alpha=0.3) +
    geom_jitter(size = 0.3, width = 0.15, height = 0) +
    labs(y="Price per meter-squared", x = "", title='Price per meter-squared vs. district') + theme_drwhy_vertical() + theme(legend.position = "none") + coord_flip()

pa5 + pa6
```

### Linear regression model {#model-Apartments-lr}

The dependent variable of interest, *m2.price*, is continuous. Thus, a natural choice to build a predictive model is the linear regression. We treat all the other variables in the `apartments` dataframe as explanatory and include them in the model. The results of the model are stored in model-object `apartments_lm_v5`.

```{r, warning=FALSE, message=FALSE}
apartments_lm_v5 <- lm(m2.price ~ ., data = apartments)
anova(apartments_lm_v5)
```

### Random forest model {#model-Apartments-rf}

As a challenger to linear regression, we consider a random forest model. To fit the model, we apply the `randomForest()` function, with default settings, from the package with the same name [@randomForest].  
The results of the model are stored in model-object `apartments_rf_v5`. 

```{r, warning=FALSE, message=FALSE, eval = FALSE}
library("randomForest")
set.seed(72)
apartments_rf_v5 <- randomForest(m2.price ~ ., data = apartments)
apartments_rf_v5
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library("randomForest")
load("models/apartments_rf_v5.rda")
apartments_rf_v5
```

### Support vector model {#model-Apartments-svm}

As an another challenger to the linear regression model, we consider a Support Vector Machines model. To fit the model, we use the `svm()` function, with default settings, from the package `e1071` [@R-e1071].  
The results of the model are stored in model-object `apartments_svm_v5`. 

```{r, warning=FALSE, message=FALSE, eval = FALSE}
library("e1071")
apartments_svm_v5 <- svm(m2.price ~ construction.year + surface + floor + 
         no.rooms + district, data = apartments)
apartments_svm_v5
```

### Model predictions {#predictionsApartments}

The `predict()` function calculates predictions for a specific model. In the example below we use model-object `apartments_lm_v5` to calculate predictions for prices for first six rows. 

```{r, warning=FALSE, message=FALSE}
predict(apartments_lm_v5, apartments_test[1:6, ])
predict(apartments_rf_v5, apartments_test[1:6, ])
```

In the example below we calculate predictive performance for `apartments_lm_v5` and `apartments_rf_v5` as the square root of the average of squared errors (RMSE). 

```{r, warning=FALSE, message=FALSE}
predicted_apartments_lm <- predict(apartments_lm_v5, apartments_test)
(rmsd_lm <- sqrt(mean((predicted_apartments_lm - apartments_test$m2.price)^2)))

predicted_apartments_rf <- predict(apartments_rf_v5, apartments_test)
(rmsd_rf <- sqrt(mean((predicted_apartments_rf - apartments_test$m2.price)^2)))

```

For the random forest model, the root-mean-square of the mean squared difference is equal to `r round(rmsd_rf, 1)`. It is almost identical as root-mean-square for the linear regression model `r round(rmsd_lm, 1)`. Thus, the question we may face is: should we choose the more complex, but flexible random-forest model, or the simpler and easier to interpret linear model? In the subsequent chapters we will try to provide an answer to this question.

As we will show in following chapters, a proper model exploration helps to understand which model we should choose. And even more, it helps to understand weak and strong sides of both models and in consequence we can create a new model better than these two.


### Model adapters {#ExplainersApartmentsRCode}

In similar spirit to the Section \@ref(ExplainersTitanicRCode) we will use explainers also for predictive models created for the `apartments` dataset.

```{r, warning=FALSE, message=FALSE, eval=FALSE}
explain_apartments_lm_v5 <- explain(model = apartments_lm_v5, 
         data = apartments_test, y = apartments_test$m2.price,
         label = "Linear Regression")
explain_apartments_rf_v5 <- explain(model = apartments_rf_v5, 
         data = apartments_test, y = apartments_test$m2.price,
         label = "Random Forest")
explain_apartments_svm_v5 <- explain(model = apartments_svm_v5, 
         data = apartments_test, y = apartments_test$m2.price,
         label = "Support Vector Machines")
```

```{r eval=FALSE, echo=FALSE}
# saveToLocalRepo(explain_apartments_lm_v5, repoDir = "models")
# [1] "78d4ee073795205905fac2c1a48fd5d0" # explainer with test data
# saveToLocalRepo(explain_apartments_rf_v5, repoDir = "models")
# [1] "b173949f8825e3f6999203270f7f68f8" # explainer with test data
# saveToLocalRepo(explain_apartments_svm_v5, repoDir = "models")
# [1] "16602ee04b662ab8e7a930d81247bab6" # explainer with test data
```

### List of objects for the `apartments` example {#ListOfModelsApartments}

In Sections \@ref(model-Apartments-lr) and \@ref(model-Apartments-rf) we have built two predictive models for the `apartments` data set. The models will be used in the rest of the book to illustrate the model explanation methods and tools. 

For the ease of reference, we summarize the models in Table \@ref(tab:archivistHooksOfModelsApartments). The binary model-objects can be downloaded by using the indicated `archivist` hooks [@archivist]. By calling a function specified in the last column of the table, one can restore a selected model in a local R environment.

Table: (\#tab:archivistHooksOfModelsApartments) Predictive models created for the `apartments` dataset. 

| Model name   | Model generator | Variables  | Archivist hooks |
|--------------|-----------------|------------|-----------------|
| `apartments_lm_v5`  | `stats:: lm` v.3.5.3  |  construction .year, surface, floor, no.rooms, district  | Get the model: `archivist:: aread("pbiecek/models/55f19")`. Get the explainer:  `archivist:: aread("pbiecek/models/78d4e")` |
|  `apartments_rf_v5` | `randomForest:: randomForest` v.4.6.14 | construction .year, surface, floor, no.rooms, district  | Get the model: `archivist:: aread("pbiecek/models/fe7a5")`. Get the explainer: `archivist:: aread("pbiecek/models/b1739")` |
|  `apartments_svm_v5` | `e1071:: svm` v.1.7-2 | construction .year, surface, floor, no.rooms, district  | Get the model: `archivist:: aread("pbiecek/models/545fa")`. Get the explainer: `archivist:: aread("pbiecek/models/16602")` |


