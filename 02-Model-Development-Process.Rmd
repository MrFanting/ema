# Model Development {#modelDevelopmentProcess}

## Introduction {#MDPIntro}

In this book we present methods that can be used for exploration and explanation of predictive models. But before we can explore a model, first we need to train one.

In this part of the book we overview the process of model development and introduce steps that lead to a model creation. It is not a comprehensive manual ,,how to train a model in 5 steps''. The goal of this chapter is to show what needs to be performed before we can do any diagnostic or exploration of a trained model.

Predictive models are created for different purposes. Sometimes it is a team of data scientists that spend months on a single model that will be used for model scoring in a big financial company. Every detail is important for models that operate on large scale and have long-term consequences. Another time it is an in-house model trained for prediction of a demand for pizza. The model is developed by a single person in few hours. If model will not perform well it will be updated, replaced or removed.

Whatever it is a large model or small one, similar steps are to be taken during model development.


## The Process {#MDPprocess}

Several approaches are proposed in order to describe the process of model development. Their main goal is to standardize the process. And the standardisation is important because it helps to plan resources needed to develop and maintain the model and also to not miss any important step.

The most known methodology for data science projects is CRISP-DM [@crisp1999], [@crisp2019wiki] which is a tool agnostic procedure. The key component of CRISP-DM is the break down of the whole process into six phases, that are iterated: business understanding, data understanding, data preparation, modeling, evaluation and deployment. CRISP-DM is general, it was designed for any data science project. For predictive models some methodologies are introduced in ,,R for Data Science'' [@r4ds2019] and ,,On XAI Misconceptions'' [@misconceptions2019].Both are focused on iterative repetitions of some phases. 
Figure \@ref(fig:MDPwashmachine) presents a variant of iterative process divided into five steps. Data preparation is needed prior to any modeling. Better data is needed for better models. On the other hand, garbage-in garbage-out. Once the data is gathered, steps that are usually highlighted are Data understanding, Model assembly and Model audit. This is the common thinking about model development. Repeat these steps until some convergence, e.g. repeat until best model is identified.

```{r MDPwashmachine, echo=FALSE, fig.cap="Lifecycle of predictive model can be decomposed into five tasks. First we need data that is poured into the model development cycle. The model development is highly iterative, learn something new about the data, assemble a new model based on current understanding, and validate the new model. Repeat these steps as long as needed to be satisfied with model performance. Once the model is created we can deliver the model to the production along with required tests and documentation.", out.width = '60%', fig.align='center'}
knitr::include_graphics("figure/MDP_washmachine.png")
```


In this book we use *Model Development Process* [@mdp2019]. It is motivated by Rational Unified Process for Software Development [@rup1998], [@usdp1999], [@spiral1988]. One can think about MDP as an extension of process introduced in Figure \@ref(fig:MDPwashmachine). What is important is to notice that consecutive iterations are not identical. Our knowledge increases during the process and consecutive iterations are performed with different goals in mind.  

This is why MDP is build as an untangled version of Figure \@ref(fig:MDPwashmachine). The MDP process is shown in Figure \@ref(fig:mdpGeneral). Each vertical stripe is a single run of the cycle. 
First iterations are usually focused on *formulation of the problem*. Sometimes the problem is well stated, however it's a rare situation valid maybe only for kaggle competitions. In most real-life problems the problem formulation requires lots of discussions and experiments. Once the problem is defined we can start building first prototypes, first *crisp versions of models*. These initial versions of models are needed to verify if the problem can be solved and how far we are form the solution. Usually we gather more information and go for the next phase, the *fine tuning*. We repeat these iterations until a final version of a model is developed. Then we move to the last phase *maintenance and* (one day) *decommissioning*.  

(ref:mdpGeneralCaption) Overview of the Model Development Process. Horizontal axis show how time passes from the problem formulation to the model decommissioning. Vertical axis shows tasks are performed in a given phase. Each vertical strip is a next iteration of cycle presented in Figure \@ref(fig:MDPwashmachine)

```{r mdpGeneral, echo=FALSE, fig.cap='(ref:mdpGeneralCaption)', out.width = '99%', fig.align='center'}
knitr::include_graphics("figure/mdp_general.png")
```

Having in mind the map of model development we can point places where one can use methods presented in this book.

As suggested in the title of this book, three primary applications are: exploration, explanation and debugging. *Exploration* refers to situations in which we better understand the data and the domain. Presented techniques can be used to speed up the variable engineering or variable selection. *Explanation* refers to situations in which we are interested in decision paths beyond particular predictions. *Debugging* refers to situations in which we want to understand weak points of a model and correct them. These applications target phases Data understanding, Model assembly and Model audit.

In this book we present various examples based on three use cases. Two introduced in Chapter \@ref(dataSetsIntro) (binary classification in surviving Titanic sinking and regression in apartments pricing) and one in Chapter \@ref(UseCaseFIFA) (estimation of soccer player value based on its skills). Due to space limitation we do not show the full life cycle of these problems, but we are focused on phases Crisp modeling and Fine tuning.

Rest of this chapter is focused on a brief overview of the notation and commonly used methods for data exploration, model training and model validation. 


## Notation {#notation}

Methods described in this book were developed by different authors, who used different mathematical notations. 
We try to keep the mathematical notation consistent throughout the entire book. In some cases this may result in formulas with a fairly complex system of indices.

In this section, we provide a general overview of the notation we use. Whenever necessary, parts of the notation will be explained again in subsequent chapters.

We assume that the data consist $n$ observations/instances. Each observation is described by $p$ explanatory variables. Thus data is described as a set of points on a $p$-dimensional input space $\mathcal X \equiv \mathcal R^p$. By $x \in \mathcal X$ we will refer to a single point in this input space.
By $x_i$ we refer to the $i$-th observation in this dataset. Of course, $x_i \in \mathcal X$. By $X$ we denote a matrix $n\times p$ with rows corresponding to consecutive observations.

Some methods of model exploration are constructed around an observation of interest which will be denoted by $x_{*}$. The observation may not necessarily belong to the analyzed dataset; hence, the use of the asterisk in the index. Of course, $x_* \in \mathcal X$.

Points in $\mathcal X$ are $p$ dimensional vectors. We refer to the $j$-th coordinate by using $j$ in superscript. Thus, $x^j_i$ denotes the $j$-th coordinate of the $i$-th observation from the analyzed dataset. If $\mathcal J$ denotes a subset of indices, then $x^{\mathcal J}$ denotes the elements of vector $x$ corresponding to the indices included in $\mathcal J$. 

We will use the notation $x^{-j}$ to refer to a vector that results from removing the $j$-th coordinate from vector $x$. By **$x^{j|=z}$**, we denote a vector with the values at all coordinates equal to the values in $x$, except of the $j$-th coordinate, which is set equal to $z$. So, if $w=x^{j|=z}$, then $w^j = z$ and $\forall_{k\neq j} w^k = x^k$. In other words $x^{j|=z} = (x^1, ..., x^{j-1}, z, x^{j+1}, ..., x^p)$.

By $x^{*j}$ we denote a matrix with the values as in $x$ except the $j$th column which is permuted.

In this book, a model is a function $f:\mathcal X \rightarrow \mathcal R$ that transforms a point from $\mathcal X$ into a real number. In most cases, the presented methods can be used directly for multivariate dependent variables; however, we use examples with uni-variate responses to simplify the notation.
Typically, during the model development, we create many competing models. Formally we shall index models to refer to a specific version of a trained model. But for the sake of simplicity we omit these indexes where they are not important.


Later in this book we will use the term **model residual** as the the difference between the observed value of the dependent variable $Y$ for the $i$-th observation from a particular dataset and the model prediction for the observation

\begin{equation}
r_i = y_i - f(x_i) = y_i - \hat y_i.
(\#eq:modelResiduals)
\end{equation}


## Data exploration 

Before we start the modeling we need to understand the data.
Visual, tabular and statistical tools for data exploration are used depending on the character of variables.

The most know introduction to data exploration is the famous book by John Tukey [@tukey1977]. It introduces new tools for data exploration, like for example boxplots or stem-and-leaf plots. Availability of computational tools makes the process of data exploration easier and more interactive. Find a good overview of techniques for data exploration in ,,Data Science in R'' [@Nolan2015] or ,,R for Data Science'' [@Wickham2017].


In this book we will rely on five visual methods for data exploration presented in Figure \@ref(fig:UMEPEDA). Two of them are used to present distribution of explanatory or target variables; three others are used to explore pairwise relations between variables.

```{r UMEPEDA, echo=FALSE, fig.cap="Basic methods for visual exploration. Histogram for distribution of continuous or categorical variables, empirical cumulative distribution for continuous variables. Mosaic plot for relation between two categorical variables, boxplots for relation between continuous and categorical variables or scatterplot for relation between two continuous variables.", out.width = '75%', fig.align='center'}
knitr::include_graphics("figure/UMEPEDA.png")
```

Distribution of categorical variable is summarized with a barplot, distribution of numerical variable is summarized with a histogram or empirical cumulative distribution function.

Primary goal for exploration of target variable is to decide if some variable transformation is needed (e.g. if the variable is skewed or with fat tails) or to verify if target variable is balanced (because some methods are not working well with unbalanced data). Exploration of dependent variables is performed mainly to decide if any variable transformation is needed.

Relations between two variables, mostly between a single dependent variable and target variable, are visualized with mosaic plots (for two categorical variables), boxplots (for numerical and categorical variable) and scatter plots (for two numerical variables). Such exploration may provide some insights for variable selection/filtering (if the variable is not related with the target then variable may be removed from the model) or variable engineering (if from the exploration we gain information how a variable may be transformed).


## Model training {#notationTraining}

In predictive modeling, we are interested in a distribution of a dependent variable $Y$ given vector $x_*$. The latter contains values of explanatory variables. In the ideal world, we would like to know the conditional distribution of $Y$ given $x_*$. In practical applications, however, we usually do not predict the entire distribution, but just some of its characteristics like the expected (mean) value, a quantile, or variance. Without loss of generality we will assume that we model the conditional expected value $E_Y(Y | x_*)$.

Assume that we have got model $f()$, for which $f(x_*)$ is an approximation of $E_Y(Y | x_*)$, i.e., $E_Y(Y | x_*) \approx f(x_*)$. Note that we do not assume that it is a "good" model, nor that the approximation is precise. We simply assume that we have a model that is used to estimate the conditional expected value and to form predictions of the values of the dependent variable. Our interest lies in the evaluation of the quality of the predictions. If the model offers a "good" approximation of the conditional expected value, it should be reflected in its satisfactory predictive performance. 

Usually the available data is split into two parts. One will be used for model training (estimation of model parameters), second will be used for model validation. The splitting may be repeated as in k-fold cross validation or repeated k-fold cross validation (see for example ,,Applied predictive modeling'' [@Kuhn2013]). We leave the topic of model validation for Chapter \@ref(modelPerformance).

Training procedures are different for different models, but most of them can be written as an optimization problem. Let $\Theta$ be a space for possible model parameters. Model training is a procedure of selection a $\theta \in \Theta$ that maximize some loss function $L(y, f_\theta(X))$. For models with large parameter spaces it is common to add additional term $\lambda(\theta)$ that control the model complexity.

\begin{equation}
\hat\theta = \arg \min_{\theta \in \Theta}  L (y, f_\theta(X)) + \lambda(\theta). 
(\#eq:modelTrainingEq1)
\end{equation}

For statistical models it is common to assume some family of probability distributions for $y|x$. In such case the loss function $L$ may be defined as a minus log-likelihood function for $\theta$. Likelihood is probability of observing $y|x$ as a function of parameter $\theta$.

For example, in linear regression we assume that that observed vector of values $y$ follow a multidimensional Gaussian distribution 
$$
y \sim \mathcal N(X \beta, I\sigma^2),
$$
where $\theta = (\beta, \sigma^2)$. In this case equation \@ref(eq:modelTrainingEq1) become

\begin{equation}
\hat\theta = \arg \min_{\theta \in \Theta}  ||y - X \beta||_{2} + \lambda(\beta). 
(\#eq:modelTrainingEq2)
\end{equation}

For linear regression, the penalty term $\lambda(\beta)$ is equal to $0$, and optimal parameters $\beta$ in equation \@ref(eq:modelTrainingEq2) have close analytical solution $\hat \beta = (X^TX)^{-1}X^Ty$. In ridge regression the penalty  $\lambda(\beta) = \lambda ||\beta||_2$ and also  \@ref(eq:modelTrainingEq2) have analytical solution $\hat \beta = (X^TX + \lambda I)^{-1}X^Ty$. For LASSO regression the penalty  $\lambda(\beta) = \lambda ||\beta||_1$ and $\beta$ are estimated through a numerical optimization.

For classification, the natural choice for distribution of $y$ is a Binomial distribution. This leads to logistic regression and logistic loss function. For multi label classification frequent choice is the cross-entropy loss function. 

Apart from linear models for $y$ there is a large variety of predictive models. Find a good overview of different techniques for model development in ,,Modern Applied Statistics with S'' [@MASSbook] or ,,Applied predictive modeling'' [@Kuhn2013].



## Model understanding

Usually the model development starts with some crisp early versions that are refined in consecutive iterations. In order to train a final model we need to try numerous candidate models that will be explored, examined and diagnosed. In this book we will introduce techniques that: 

* summarise how good is the current version of a model. Section \@ref(modelPerformance) overviews measures for model performance. These measures are usually used to trace the progress in model development.
* assess the feature importance. Section \@ref(featureImportance) shows how to assess influence of a single variable on model performance. Features that are not important are usually removed from a model during the model refinement. 
* shows how a single feature affects the model response. Sections \@ref(partialDependenceProfiles) -- \@ref(accumulatedLocalProfiles) present Partial Dependence Profiles, Accumulated Local Effects and Marginal Profiles. All these techniques help to understand how model consumes particular features. 
* identifies potential problems with a model. Section \@ref(residualDiagnostic) shows techniques for exploration of model residuals. Looking closer on residuals often help to improve the model. This is possible with tools for local model exploration which are presented in the fist part of the book.
* performs sensitivity analysis for a model. Section \@ref(ceterisParibus) introduces Ceteris Paribus profiles that helps in a what-if analysis for a model.
* validated local fit for a model. Section \@ref(localDiagnostics) introduces techniques for assessment if for a single observation the model support its prediction.
* decompose model predictions into pieces that can be attributed to particular variables.  Sections \@ref(breakDown) -- \@ref(LIME) show different techniques like SHAP, LIME or Break Down for local exploration of a model.

