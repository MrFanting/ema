# Residual Diagnostics {#residualDiagnostic}

## Introduction {#IntroResidualDiagnostic}

In this chapter, we present methods that are useful for detailed examination of both overall and instance-specific model performance. In particular, we focus on graphical methods that use residuals. The methods may be used for several purposes:

* In the first part of the book, we discussed tools for single-instance examination. Residuals can be used to identify potentially problematic instances. The single-instance explainers can then be used in the problematic cases to understand, for instance, which factors contribute most to the errors in prediction.

* For most models, residuals should express a random behavior with certain properties (like, e.g., being concentrated around 0). If we find any systematic deviations from the expected behavior, they may signal an issue with a model (like, e.g., an omitted explanatory variable or wrong functional form of a variable included in the model). 

* In Chapter \@ref(modelPerformance) we discussed measures to evaluate the overall performance of a predictive model. Sometimes, however, we may be more interested in cases with the largest errors of prediction, which can be identified with the help of residuals.

Residual diagnostics is a classical topic related to statistical modeling. Literature on the topic is vast -- essentially every book on statistical modeling includes some discussion about residuals. Thus, in this chapter, we are not aiming at being exhaustive. Rather, our goal is to present selected concepts that underlie the use of residuals. 

## Intuition {#IntuitionResidualDiagnostic}

As we mentioned in Section \@ref(notationTraining), we primarily focus on models describing the expected value of the dependent as a function of explanatory variables. In such case, for a perfect predictive model, the predicted value of the dependent variable should be exactly equal to the actual value of the variable for every observation. Perfect prediction is rarely, if ever, expected. In practice, we want the predictions to be reasonably close to the actual values. This suggests that we can use the difference between the predicted and the actual value of the dependent variable to quantify the quality of predictions obtained from a model. The difference is called a residual.

For a single observation, residual will almost always be different from zero. While a large (absolute) value of a residual may indicate a problem with a prediction for a particular observation, it does not mean that the quality of predictions obtained from a model is unsatisfactory in general. To evaluate the quality, we should investigate the ''behavior'' of  residuals for a group of observations. In other words, we should look at the distribution of the values of residuals. 

For a ''good'' model, residuals should deviate from zero randomly, i.e., not systematically. Thus, their distribution should be symmetric around zero, implying that their mean (or median) value should be zero. Also, residuals should be close to zero themselves, i.e., they should show low variability. 

Usually, to verify these properties, graphical methods are used. For instance, a histogram of can be used to check the symmetry and location of the distribution of the residuals. For linear regression models, a plot of residuals against a continuous covariate can be checked for absence of any patterns that would suggest any systematic error in the predictions obtained for a specific range of the values of the covariate.

Note that a model may imply a concrete distribution for residuals. For instance, in the case of the classical linear regression model, standardized residuals should be normally distributed with mean zero and a constant variance. In such a case, a the distributional assumption can be verified by using a suitable graphical method like, for instance, a quantile-quantile plot. If the assumption is found to be violated, one might want to be careful regarding to predictions obtained from the model.

```{r residuals1234, echo=FALSE, fig.cap="Diagnostic plots for linear models. Consecutive panels present residuals as a function of fitted values, standardized residuals as a function of fitted values, leverage plot and qq-plot.", out.width = '100%', fig.align='center'}
knitr::include_graphics("figure/residuals1234.png")
```

## Method {#MethodResidualDiagnostic}

As it was already mentioned in Chapter \@ref(modelDevelopmentProcess), for a continuous dependent variable (or a count), residual $r_i$ for the $i$-th observation in a dataset is the difference between the model prediction and the corresponding value of the variable:

\begin{equation}
r_i = y_i - f(x_i).
(\#eq:resid)
\end{equation}

A histogram of the estimated residuals can be used to check the symmetry and location of their distribution. An index plot of residuals, i.e., the plot of residuals against the corresponding observation number, may be used to identify observations with large residuals.

For diagnostic purposes the *standardized residuals* are defined as

\begin{equation}
\tilde{r}_i = \frac{r_i}{\sqrt{\mbox{Var}(r_i)}},
(\#eq:standresid)
\end{equation}

where $\mbox{Var}(r_i)$ is the variance of the residual $r_i$. 
Of course, in practice, the variance of $r_i$ is usually unknown. Hence, in Equation \@ref(eq:standresid), the estimated value of the variance is used. Residuals defined in this way are often called the *Pearson residuals*.

For a gaussian linear model the $\mbox{Var}(r_i)$ can be estimated from design matrix and the distribution of $\tilde{r}_i$ is approximately standard normal. In general case, for complicated models, it is hard to estimate the variance $\mbox{Var}(r_i)$ for a single instance so it is often approximated by a constant. 

<!-- Consider, for example, the classical linear-regression model. According to the model, for the $i$-th observation, the dependent variable $Y_i$ should follow a normal distribution with mean $f(x_i)$ and variance $\sigma^2$. Thus, the standardized residual $\tilde{r}_i = r_i/\sigma$. If the model is correct, $\tilde{r}_i$ should have a standard-normal distribution. The Pearson residual is defined as $r_i/\widehat{\sigma}$, where $\widehat{\sigma}$ is the estimated value of $\sigma$, and has, approximately, also a standard-normal distribution.  -->

Definition \@ref(eq:standresid) can also be applied to a binary dependent variable if the model prediction $f(x_i)$ is the probability observing $y_i$ and upon coding the two possible values of the variable as 0 and 1. However, in this case, the range of possible values of $r_i$ is restricted to $(-1,1)$, which limits the usefulness of the residuals. For this reason, more often the Pearson residuals are used. Note that, if the values of the explanatory-variable vectors $x_i$ lead to different predicted values $f(x_i)$ for different observations in a  dataset, the distribution of the Pearson residuals will not be approximated by the standard normal one. Nevertheless, the index plot may still be useful to detect observations with large residuals. The standard-normal approximation is more likely to apply in the situation when vectors $x_i$ split data into (a few, perhaps) groups sharing the same predicted value $f(x_i)$. In that case, one one can consider averaging residuals $r_i$ per group and standardizing them by $\sqrt{f(x_i)\{1-f(x_i)/k\}}$, where $k$ is the number of observations in a particular group. 

For categorical data, residuals can only be defined in terms of residuals of the binary variables indicating the category observed for the $i$-th observation.

<!--
[TOMASZ: NOT SURE IF THE BELOW IS WORTH INCLUDING. WE DO NOT SEEM TO LOOK AT QQ PLOTS. PERHAPS WE SHOULD?]

For models implying concrete distributional assumptions one can define a generalization of residuals called *pseudo-residuals* [Zucchini, MacDonald, Langrock "Hidden Markov Models for Time Series. An Introduction Using R. (
Second Edition)]. Assume that, according to the model, the dependent variable for the $i$-th observation should have a distribution with the cumulative distribution function $F_i(y)$, i.e., $P(Y_i \leq y) = F_i(y)$. In that case, the *uniform pseudo-residual* is defined as follows:

$$
u_i=F_i(y_i).
$$

If the model correctly describes the distribution of the dependent variable, then the pseudo-residual, obtained by transforming the variable by its cumulative distribution function, should have a uniform distribution on the interval $[0,1]$. A histogram of the estimated residuals could be used to check the uniform-distribution assumption.

Note that a value of $u_i$ close to 0 or 1 indicates that $y_i$ is located in one of the tails of the model-predicted distribution, i.e., it is unlikely from the point of view of the distribution. Excess of such values would indicate a systematic failure of the model to provide good predictions.  

However, uniform pseudo-residuals are not very useful for detection of observations for which model fails to yield good predictions. This is because it is difficult to discriminate between values very close to 0 or 1, like 0.975 and 0.999. For this reason, often, *normal pseudo-residuals* are used. They are defined as follows:

$$
\tilde{u}_i=\Phi^{-1}(u_i),
$$

where $\Phi()$ denotes the cumulative distribution function of the standard normal distribution. If the model is correct, normal pseudo-residuals should have the standard-normal distribution. A quantile-quantile plot of the estimated residuals could be used to check the normal-distribution assumption. Note that, for $u_i=0.975$ and 0.999, we get clearly distinguishable values of $\tilde{u}_i=1.96$ and 3.09, respectively. 

It is worth noting that the normal pseudo-residual measures, in general, the deviation of the observed value of the dependent variable from the model-predicted median of the corresponding distribution. This is because 

\begin{equation}
\tilde{u}_i=0 \rightarrow \Phi^{-1}(u_i)=0  \rightarrow u_i=\Phi(0)=\frac{1}{2} \rightarrow F_i(y_i)=\frac{1}{2}.
(\#eq:pseudomedian)
\end{equation}

The last equality in \@ref(eq:pseudomedian) implies that $y_i$ is equal to the median of the corresponding distribution of the dependent variable. 

Consider, for example, the classical linear-regression model. In this case, $F_i$ is the cumulative distribution function of a normal distribution with mean $f(x_i)$ and variance $\sigma^2$. It follows that the uniform pseudo-residual is equal to

$$
u_i=F_i(y_i)=\Phi\left\{\frac{y_i-f(x_i)}{\sigma}\right\}=\Phi\left(\frac{r_i}{\sigma}\right)=\Phi\left(\tilde{r_i}\right).
$$

If the model correctly captures the distribution of the dependent variable, the standardized residual $\tilde{r}_i$ should follow a standard-normal distribution. Consequently, $u_i$, obtained by transforming $\tilde{r}_i$ by its cumulative distribution function $\Phi()$, should follow a uniform distribution, as required for the uniform pseudo-residual.

On the other hand, the normal pseudo-residual for the linear-regression model is given by

$$
\tilde{u}_i=\Phi^{-1}\{F_i(y_i)\}=\Phi^{-1}\left\{\Phi\left(\frac{r_i}{\sigma}\right)\right\}=\tilde{r_i}.
$$
Thus, in this case, the normal pseudo-residual is simply the standardized residual. Consequently, $\tilde{u}_i$ should follow a standard-normal distribution, as required for the normal pseudo-residual. Moreover, $\tilde{u}_i$ measures the deviation of the observed value of the dependent variable from the model-predicted mean (equal to the median) of the corresponding (normal) distribution.

For a discrete dependent variable, e.g., a count, one can define the *uniform pseudo-residual segments* as follows:

$$
[u^-_i,u^+_i]=[F_i(y^-_i),F_i(y_i)],
$$

where $y^-_i$ is the largest possible value of the dependent variable smaller than $y_i$. Essentially, the length of the interval $[u^-_i,u^+_i]$ is equal to $P(Y_i=y_i)$, the probability of observing $y_i$. Thus, the uniform pseudo-residual segment provides an information on how rare the observed value of the dependent variable is given the corresponding distribution predicted by the model.   

The *normal pseudo-residual segment* is defined as 

$$
[\tilde{u}^-_i,\tilde{u}^+_i]=[\Phi^{-1}\{ F_i(y^-_i)\},\Phi^{-1}\{F_i(y_i)\}].
$$

Essentially, the limits of the interval $[\tilde{u}^-_i,\tilde{u}^+_i]$ are the quantiles of the standard-normal distribution corresponding to $P(Y_i\leq y^-_i)$ and $P(Y_i\leq y_i)$, respectively, so that 

$$
\Phi(\tilde{u}_i)-\Phi(\tilde{u}^-_i)=P(Y_i\leq y_i)-P(Y_i\leq y^-_i)=P(Y_i= y_i).
$$

Furthermore, one can define *normal mid-pseudo-residual* as follows:

$$
\bar{\tilde{u}}_i=\Phi^{-1}\left(\frac{\tilde{u}^-_i+\tilde{u}^+_i}{2}\right).
$$

The normality of estimated mid-pseudo-residuals can be checked by constructing a histogram or a normal quantile-quantile plot.
-->

## Example: Apartments data {#ExampleResidualDiagnostic}

In this section, we use the linear-regression model `apartments_lm_v5` (Section \@ref(model-Apartments-lr)) and the random-forest model `apartments_rf_v5` (Section \@ref(model-Apartments-rf)) for the apartment-prices data (Section \@ref(ApartmentDataset)). Recall that the dependent variable of interest, the price per square-meter, is continuous. Thus, we can use residuals $r_i$, as defined in equation \@ref(eq:resid). It is worth noting that, as it was mentioned in Section \@ref(modelPerformanceApartments), RMSE for both models is very similar. Thus, overall, the two models could be seen as performing similarly.

```{r modelResidualsArchivistRead0, warnings=FALSE, echo=FALSE, message=FALSE}
library("DALEX")
library("ggplot2")
library("randomForest")

explainer_apartments_lr <- archivist:: aread("pbiecek/models/f49ea")
explainer_apartments_rf <- archivist:: aread("pbiecek/models/569b0")

explainer_apartments_lr <- explain(explainer_apartments_lr$model,
    explainer_apartments_lr$data,
    explainer_apartments_lr$data$m2.price, 
    verbose = FALSE)
explainer_apartments_rf <- explain(explainer_apartments_rf$model,
    explainer_apartments_rf$data,
    explainer_apartments_rf$data$m2.price, 
    verbose = FALSE)

mr_lr <- DALEX::model_performance(explainer_apartments_lr)
mr_rf <- DALEX::model_performance(explainer_apartments_rf)
```

Figures \@ref(fig:plotResidualDensity1) and \@ref(fig:plotResidualBoxplot1) summarize the distribution of residuals for both models. In particular, Figure \@ref(fig:plotResidualDensity1) presents histogram of residuals, while Figure \@ref(fig:plotResidualBoxplot1) shows box-and-whisker plots for the absolute value of the residuals. 

(ref:plotResidualDensity1Desc) Histogram of residuals  the linear-regression model `apartments_lm_v5` and the random-forest model `apartments_rf_v5` for the `apartments` data.

```{r plotResidualDensity1, fig.cap='(ref:plotResidualDensity1Desc)',  warning=FALSE, message=FALSE, fig.width=7, fig.height=4,  fig.align='center', echo=FALSE}
plot(mr_lr, mr_rf, geom = "histogram") + scale_x_continuous(breaks = seq(-1000,1600,200))
```

(ref:plotResidualBoxplot1Desc) Box-and-whisker plots of the absolute values of the residuals of the linear-regression model `apartments_lm_v5` and the random-forest model `apartments_rf_v5` for the `apartments` data. The crosses indicate the average value that corresponds to RMSE.

```{r plotResidualBoxplot1, fig.cap='(ref:plotResidualBoxplot1Desc)',  warning=FALSE, message=FALSE, fig.width=7, fig.height=2.5,  fig.align='center', echo=FALSE}
plot(mr_lr, mr_rf, geom = "boxplot") + scale_y_continuous(breaks = seq(-1000,1600,200))
```

Despite the similar value of RMSE, the distribution of residuals for both models is different. In particular, Figure \@ref(fig:plotResidualDensity1) indicates that the distribution for the linear-regression model is, in fact, split into two separate, normal-like parts, which may suggest omission of a binary explanatory variable in the model. The two components are located around the values of about -200 and 400. As mentioned in the previous chapters, the reason for this behavior of the residuals is the fact that the model does not capture the non-linear relationship between the price and the year of construction. 

As seen from Figure \@ref(fig:plotResidualDensity1), the distribution for the random-forest model is skewed to the right and multimodal. It seems to be centered at a value closer to zero than the  distribution for the linear-regression model, but it shows a larger variation. These conclusions are confirmed by the box-and-whisker plots in Figure \@ref(fig:plotResidualBoxplot1). 

The two plots suggest that the residuals for the random-forest model are more frequently smaller than the residuals for the linear-regression model. However, a fraction of the random-forest-model residuals are very large and these large residuals result in the RMSE being comparable for the two models.


In the remainder of the section, we focus on the random-forest model. 

Figure \@ref(fig:plotResidual1) shows a scatterplot of residuals (y-axis) in function of the observed (x-axis) values of the dependent variable. For a perfect predictive model, we would expect the horizontal line at zero. For a ''good'' model, we would like to see a symmetric scatter of points around the horizontal line at zero, indicating random deviations of predictions from the observed values. The plot in Figure \@ref(fig:plotResidual1) shows that, for the large observed values of the dependent variable, the residuals are positive, while for small values they are negative. Thus, the plot suggests that the predictions are shifted (biased) towards the average. 

(ref:plotResidual1Desc) Residuals and observed values of the dependent variable for the random-forest model `apartments_rf_v5` for the `apartments` data.

```{r plotResidual1, fig.cap='(ref:plotResidual1Desc)',  warning=FALSE, message=FALSE, fig.width=7, fig.height=5,  fig.align='center', echo=FALSE}
md_lr <- model_diagnostics(explainer_apartments_lr)
md_rf <- model_diagnostics(explainer_apartments_rf)

plot(md_rf, variable = "y", yvariable = "residuals") + xlab("true price") + ylab("residuals")
```

The shift towards the average can also be seen from Figure \@ref(fig:plotPrediction1) that shows a scatterplot of the predicted (y-axis) and observed (x-axis) values of the dependent variable. For a perfect predictive model we would expect a diagonal line (marked in red). The plot shows that, for large observed values of the dependent variable, the predictions are smaller than the observed values, with an opposite trend for the small observed values of the dependent variable. 

(ref:plotPrediction1Desc) Predicted and observed values of the dependent variable for the random-forest model `apartments_rf_v5` for the `apartments` data. Red line is the diagonal.

```{r plotPrediction1, fig.cap='(ref:plotPrediction1Desc)',  warning=FALSE, message=FALSE, fig.width=7, fig.height=5,  fig.align='center', echo=FALSE}
plot(md_rf, variable = "y", yvariable = "y_hat") + xlab("true price") + ylab("predicted price")

```

Figure \@ref(fig:plotResidual2) shows an index plot of residuals, i.e., their scatterplot in function of an (arbitrary) id-number of the observation (x-axis). The plot indicates an asymmetric distribution of residuals around zero, as there is an excess of large positive (larger than 500) residuals without a corresponding fraction of negative values. This can be linked to the right-skewed distribution seen in Figures \@ref(fig:plotResidualDensity1) and \@ref(fig:plotResidualBoxplot1) for the random-forest model. 

(ref:plotResidual2Desc) An index plot of residuals for the random-forest model `apartments_rf_v5` for the `apartments` data.

```{r plotResidual2, fig.cap='(ref:plotResidual2Desc)',  warning=FALSE, message=FALSE, fig.width=7, fig.height=5,  fig.align='center'}
plot(md_rf, variable = "ids", yvariable = "residuals") + 
    xlab("observation id") + ylab("residuals")
```

Figure \@ref(fig:plotResidual3) shows a scatterplot of residuals (y-axis) in function of the predicted (x-axis) value of the dependent variable. For a ''good'' model, we would like to see a symmetric scatter of points around the horizontal line at zero. The plot in Figure \@ref(fig:plotResidual3), as the one in Figure \@ref(fig:plotResidual1),  the plot suggests that the predictions are shifted (biased) towards the average. 

(ref:plotResidual3Desc) Residuals and predicted values of the dependent variable for the random-forest model `apartments_rf_v5` for the `apartments` data.

```{r plotResidual3, fig.cap='(ref:plotResidual3Desc)',  warning=FALSE, message=FALSE, fig.width=5, fig.height=5,  fig.align='center'}
plot(md_rf, variable = "y_hat", yvariable = "residuals") + 
    xlab("predicted price") 

```

<!--
Figure \@ref(fig:plotAutocorrelation1) presents an autocorrelation plot of residuals, i.e., the scatterplot of the residual for the $i+1$-th observation in function of the residual for the $i$-th observation. For a ''good'' model, the plot should not exhibit any pattern. The plot in Figure \@ref(fig:plotAutocorrelation1) does show a substantial positive (auto)correlation. 
-->

(ref:plotAutocorrelation1Desc) Autocorrelation plot of residuals for the random-forest model `apartments_rf_v5` for the `apartments` data. Residual for the (i+1)-th observation (y-axis) plotted in function of residual for the i-th observation.

```{r plotAutocorrelation1, fig.cap='(ref:plotAutocorrelation1Desc)',  warning=FALSE, message=FALSE, fig.width=5, fig.height=5, fig.align='center', eval=FALSE, echo=FALSE}
plot_autocorrelation(mr_rf)
```

The random-forest model, as the linear-regression model, assumes that residuals should be homoscedastic, i.e., that they should have a constant variance. Figure \@ref(fig:plotScaleLocation1) presents the scale-location plot of residuals, i.e., a scatterplot of the absolute value of  residuals in function of the predicted values of the dependent variable. The plot includes a smoothed line capturing the average trend. For homoscedastic residuals, we would expect a symmetric scatter around a horizontal line, for which the smoothed trend should be also horizontal. The plot in Figure \@ref(fig:plotScaleLocation1) deviates from the expected pattern and indicates that the variability of the residuals depends on the (predicted) value of the dependent variable.    

(ref:plotScaleLocation1Desc) The scale-location plot of residuals for the random-forest model `apartments_rf_v5` for the `apartments` data. The square-roots of the absolute values of standardized residuals (y-axis) are plotted in function of the predicted values of the dependent variable (x-axis).

```{r plotScaleLocation1, fig.cap='(ref:plotScaleLocation1Desc)',  warning=FALSE, message=FALSE, fig.width=5, fig.height=5,  fig.align='center'}
plot(md_rf, variable = "y_hat", yvariable = "abs_residuals") + 
    xlab("predicted price") + ylab("|residuals|")
```


## Pros and cons {#ProsConsResidualDiagnostic}

Diagnostics of the residuals is a very important stage of model exploration. 
Properly performed diagnostics allows to identify many different types of problems such as:

* Bias in predictions for instances with extremely high values of the target variable.
* The heterogeneous variance of the residuals, suggesting perhaps incorrect specification of the model.
* High values of residual for some ranges of a variable suggesting an incorrect model specification for some subgroup of observations.


However, the problem with diagnostics is that there is lots of diagnostic charts to review. And without quantitative measures of meeting assumption, we can only rely on the organoleptic review of the graph after the graph.


## Code snippets for R {#RcodeResidualDiagnostic}

In this section, we present the key features of the `DALEX`  package which is a wrapper for functions from `auditor` package [@R-auditor]. This package covers all methods presented in this chapter. 

First, we load explainers for the linear-regression model `apartments_lm_v5` and the random-forest model `apartments_rf_v5` created in Section \@ref(ExplainersApartmentsRCode) for the `apartments` data.

```{r modelResidualsArchivistReadcode, message=FALSE, eval=FALSE}
library("DALEX")
library("randomForest")

explainer_apartments_lr <- archivist:: aread("pbiecek/models/f49ea")
explainer_apartments_rf <- archivist:: aread("pbiecek/models/569b0")
```

There are two functions that will be used for exploration of residuals.
The `DALEX::model_performance()` function is useful for exploration of distribution of residuals while `DALEX::model_diagnostics()` function is useful for looking for relation between residuals and other variables.

Let's start with distributions of residuals. This can be done with the `model_performance()` function. The residuals are stored in separate objects that can be used for construction of various plots and summaries.

```{r modelResidualscode, message=FALSE}
mr_lr <- DALEX::model_performance(explainer_apartments_lr)
mr_rf <- DALEX::model_performance(explainer_apartments_rf)
```

The generic `plot()` function shows different statistics based on specified `geom` argument. In particular, Figure \@ref(fig:plotResidualDensity1) can be constructed by using the following simple code:

```{r plotResidualDensity1code, warning=FALSE, message=FALSE, eval=FALSE}
plot(mr_lr, mr_rf, geom = "histogram") 
```

Note that, by including the two objects containing residuals for the linear-regression model and the random-forest model in the function call, we automatically get an overlay of the plots of the histogram of residuals for the two models.

The box-and-whisker plots of the residuals for the two models, shown in Figure \@ref(fig:plotResidualBoxplot1), can be constructed by using the following simple call with `geom = "boxplot"`.

```{r plotResidualBoxplot1code,  warning=FALSE, message=FALSE,eval=FALSE}
plot(mr_lr, mr_rf, geom = "boxplot")
```

Function `model_diagnostics()` calculates residuals for various scatter plots of residuals against some other variables

```{r plotResidual1code, warning=FALSE, message=FALSE, eval=FALSE}
md_lr <- model_diagnostics(explainer_apartments_lr)
md_rf <- model_diagnostics(explainer_apartments_rf)
```

The generic `plot()` function produces a scatterplot of residuals (y-axis) in function of the observed (x-axis) values of the dependent variable, as in Figure \@ref(fig:plotResidual1). By using arguments `variable` and `yvariable`, one specify which variables will be plotted on OX and OY axis. Apart of variables names, one can use following constants:

* `y` for true target values,
* `y_hat` for predicted target values,
* `obs` for ids of an observation,
* `residuals` for calculated residual,
* `abs_residuals` for absolute values of residual.

For example, to reproduce Figure \@ref(fig:plotResidual1) one needs to plot target variable on the OX axis and residuals on OY.

```{r plotResidual2code,  warning=FALSE, message=FALSE, eval=FALSE}
plot(md_rf, variable = "y", yvariable = "residuals") 
```

To produce Figure \@ref(fig:plotPrediction1) we need to plot predicted target values on OY axis. This can be done with `yvariable = "y_hat"` argument. 

```{r plotResidual3code, warning=FALSE, message=FALSE, eval=FALSE}
plot(md_rf, variable = "y", yvariable = "y_hat") 
```

The Figure \@ref(fig:plotResidual2) has indexes of observations on OX axis. This can be achieved with `variable = "ids"` argument.

```{r plotPrediction1code, warning=FALSE, message=FALSE, eval=FALSE}
plot(md_rf, variable = "ids", yvariable = "residuals")
```

In the Figure \@ref(fig:plotScaleLocation1) on OY scale we plotted absolute residuals. This can be done with `yvariable = "abs_residuals"` argument.

```{r plotAutocorrelation1code, warning=FALSE, message=FALSE, eval=FALSE}
plot(md_rf, variable = "y_hat", yvariable = "abs_residuals")
```

